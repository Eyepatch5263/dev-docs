[
    {
        "id": "ai-ml-1",
        "question": "What is the difference between supervised and unsupervised learning?",
        "answer": "Supervised learning uses labeled data where the model learns from input-output pairs (e.g., classification, regression). Unsupervised learning uses unlabeled data to find patterns and structures (e.g., clustering, dimensionality reduction).",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-2",
        "question": "What is overfitting and how can you prevent it?",
        "answer": "Overfitting occurs when a model learns training data too well, including noise, leading to poor generalization. Prevention methods: regularization (L1/L2), dropout, early stopping, cross-validation, and using more training data.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-3",
        "question": "Explain the bias-variance tradeoff.",
        "answer": "Bias is error from incorrect assumptions (underfitting), variance is error from sensitivity to training data fluctuations (overfitting). High bias = simple model, high variance = complex model. Goal is to balance both for optimal performance.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-4",
        "question": "What is gradient descent and how does it work?",
        "answer": "Gradient descent is an optimization algorithm that iteratively adjusts model parameters to minimize a loss function. It calculates the gradient (slope) of the loss and moves in the opposite direction. Variants: batch, stochastic (SGD), and mini-batch.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-5",
        "question": "What is the difference between precision and recall?",
        "answer": "Precision = TP/(TP+FP) - proportion of positive predictions that are correct. Recall = TP/(TP+FN) - proportion of actual positives correctly identified. High precision = few false positives, high recall = few false negatives.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-6",
        "question": "What is a neural network activation function and why is it needed?",
        "answer": "Activation functions introduce non-linearity, allowing neural networks to learn complex patterns. Common ones: ReLU (f(x)=max(0,x)), Sigmoid (0-1 range), Tanh (-1 to 1), Softmax (probability distribution). Without them, networks would only learn linear relationships.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-7",
        "question": "Explain backpropagation in neural networks.",
        "answer": "Backpropagation computes gradients of the loss function with respect to each weight by applying the chain rule backwards through the network. It efficiently calculates how much each weight contributed to the error, enabling gradient descent optimization.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-8",
        "question": "What is the difference between bagging and boosting?",
        "answer": "Bagging (Bootstrap Aggregating) trains multiple models in parallel on random subsets, then averages predictions (e.g., Random Forest). Boosting trains models sequentially, each focusing on previous errors (e.g., AdaBoost, XGBoost). Bagging reduces variance, boosting reduces bias.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-9",
        "question": "What is cross-validation and why is it important?",
        "answer": "Cross-validation splits data into k folds, training on k-1 folds and validating on the remaining fold, rotating through all folds. It provides a more reliable estimate of model performance than a single train-test split and helps detect overfitting.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-10",
        "question": "Explain the concept of transfer learning.",
        "answer": "Transfer learning uses a pre-trained model (trained on large dataset) as a starting point for a new task. The model's learned features are transferred, requiring less data and training time. Common in computer vision (ImageNet models) and NLP (BERT, GPT).",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-11",
        "question": "What is the vanishing gradient problem?",
        "answer": "In deep networks, gradients become extremely small during backpropagation through many layers, making early layers learn very slowly. Solutions: ReLU activation, batch normalization, residual connections (ResNet), LSTM/GRU for RNNs.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-12",
        "question": "What is the difference between L1 and L2 regularization?",
        "answer": "L1 (Lasso) adds sum of absolute weights to loss, promotes sparsity (many weights become zero), useful for feature selection. L2 (Ridge) adds sum of squared weights, distributes penalty across all weights, prevents large weights. L1 creates sparse models, L2 creates small weights.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-13",
        "question": "What is batch normalization and why is it useful?",
        "answer": "Batch normalization normalizes layer inputs across a mini-batch (mean=0, variance=1), then scales and shifts with learnable parameters. Benefits: faster training, higher learning rates, reduces internal covariate shift, acts as regularization, less sensitive to initialization.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-14",
        "question": "Explain the attention mechanism in transformers.",
        "answer": "Attention allows models to focus on relevant parts of input when processing each element. It computes weighted sums of values based on query-key similarity scores. Self-attention relates different positions in a sequence. Transformers use multi-head attention for parallel processing of different representation subspaces.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-15",
        "question": "What is the difference between a CNN and RNN?",
        "answer": "CNNs (Convolutional Neural Networks) use convolution layers for spatial data (images), detecting local patterns through filters. RNNs (Recurrent Neural Networks) process sequential data (text, time series) with loops that maintain hidden state. CNNs for spatial, RNNs for temporal patterns.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-16",
        "question": "What is dropout and how does it prevent overfitting?",
        "answer": "Dropout randomly deactivates neurons during training (typically 20-50%), forcing the network to learn redundant representations. This prevents co-adaptation of neurons and acts as ensemble learning. At inference, all neurons are used with scaled weights.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-17",
        "question": "Explain the concept of embedding in NLP.",
        "answer": "Embeddings map discrete tokens (words, characters) to continuous vector spaces where semantic similarity is captured by distance. Word2Vec, GloVe learn static embeddings. Contextual embeddings (BERT, GPT) generate different vectors based on context. Reduces dimensionality and captures meaning.",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-18",
        "question": "What is the difference between generative and discriminative models?",
        "answer": "Generative models learn P(X,Y) - joint probability of inputs and outputs, can generate new samples (e.g., GANs, VAEs, Naive Bayes). Discriminative models learn P(Y|X) - conditional probability, focus on decision boundaries (e.g., logistic regression, SVM, neural networks).",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-19",
        "question": "What is a confusion matrix and what metrics can you derive from it?",
        "answer": "A confusion matrix shows TP, TN, FP, FN for classification. Derived metrics: Accuracy = (TP+TN)/Total, Precision = TP/(TP+FP), Recall = TP/(TP+FN), F1-Score = 2*(Precision*Recall)/(Precision+Recall), Specificity = TN/(TN+FP).",
        "category": "ai-ml"
    },
    {
        "id": "ai-ml-20",
        "question": "What is the difference between SGD, Adam, and RMSprop optimizers?",
        "answer": "SGD updates weights using gradients with fixed learning rate. RMSprop adapts learning rate per parameter using moving average of squared gradients. Adam combines momentum (moving average of gradients) and RMSprop (adaptive learning rates). Adam is most popular for its robustness and fast convergence.",
        "category": "ai-ml"
    }
]